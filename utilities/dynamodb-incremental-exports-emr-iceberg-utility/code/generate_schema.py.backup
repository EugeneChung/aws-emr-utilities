# Import necessary libraries
from pyspark.sql import SparkSession
import sys
import json
import boto3
from collections import OrderedDict

def generate_simple_dynamodb_schema(field):
    # Check the nested struct's field name
    if 'StructType' in str(field.dataType):
        nested_field = field.dataType.fields[0]
        nested_field_name = nested_field.name
        if nested_field_name == "S":
            return "S"
        elif nested_field_name == "N":
            return "N"
    elif "BooleanType" in str(field.dataType):
        return "BOOL"
    elif "DateType" in str(field.dataType) or "TimestampType" in str(field.dataType):
        return "S"  # Represent dates/timestamps as strings in DynamoDB
    elif "MapType" in str(field.dataType):
        return "M"  # Represent maps in DynamoDB
    elif "ArrayType" in str(field.dataType):
        return "L"  # Represent lists in DynamoDB
    else:
        return "S"  # Default to string type for any other unhandled type

# Check if the correct number of command-line arguments are provided
if len(sys.argv) != 3:
    print("Usage: generate_schema.py <input_s3_path> <output_s3_path>")
    sys.exit(1)
# Extract input S3 path and output S3 path from command-line arguments
input_s3_path = sys.argv[1]
output_s3_path = sys.argv[2]
# Initialize a Spark session
spark = SparkSession.builder.appName("Nested JSON Schema Inference").getOrCreate()
# Read the JSON data from the given S3 path into a DataFrame
df = spark.read.json(input_s3_path)
df.printSchema()
# Assuming the "Item" field is always present, let's extract the nested schema
item_schema = df.schema["Item"].dataType
print(item_schema)
# Generate a simplified DynamoDB-compatible schema from the DataFrame's schema for the nested "Item" attributes
# Using OrderedDict to retain the original order of columns
simple_schema = OrderedDict([(field.name, generate_simple_dynamodb_schema(field)) for field in item_schema.fields])
# Convert the simple schema to JSON string
schema_str = json.dumps(simple_schema, indent=4)
# Print the inferred schema
print(f"Inferred schema: \n{schema_str}")
# Extract bucket and key for output path
output_bucket = output_s3_path.split('/')[2]
output_key = '/'.join(output_s3_path.split('/')[3:])
# Use boto3 to upload the schema string to the specified S3 path
s3 = boto3.client('s3')
s3.put_object(Bucket=output_bucket, Key=output_key, Body=schema_str)
# Print a success message with the output path
print(f"Schema saved to {output_s3_path}")
# Gracefully stop the Spark session
spark.stop()

